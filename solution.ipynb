{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тестовое задание \"МигКредит\"\n",
    "\n",
    "## Цель:\n",
    "Построить модель логистической регрессии.\n",
    "\n",
    "## Описание данных:\n",
    "\n",
    "* Таблица buffer_table1\n",
    "\n",
    "|столбец|описание|\n",
    "|---|---|\n",
    "|ucdb_id|уникальный id наблюдения|\n",
    "|train|тип сэмпла, принимает 2 значения True и False. False – строки без разметки.|\n",
    "|30+ 3MoB gt 0|целевое событие, 1 или 0, пустое там, где train – False|\n",
    "|cash_hist|техническое поле, по которому нужно производить merge с остальными файлами|\n",
    "\n",
    "* Таблица buffer_table2\n",
    "\n",
    "|столбец|описание|\n",
    "|---|---|\n",
    "|Request_id|поле, к которому производится merge|\n",
    "|Остальные поля – характеристики кредитной истории заемщика|\n",
    "\n",
    "* Таблица buffer_table3\n",
    "\n",
    "|столбец|описание|\n",
    "|---|---|\n",
    "|Request_id|поле, к которому производится merge.|\n",
    "|Остальные поля|характеристики запросов кредитной истории заемщика.|\n",
    "\n",
    "Baseline – > 45 gini на train и test’е\n",
    "\n",
    "Прислать нужно csv файл с 2 столбцами, 1 – ucdb_id, 2 – predict_proba – столбец с вероятностями.\n",
    "\n",
    "Request id это уникальный ид, по которому нужно делать merge, с исходником\n",
    "create_tstamp это время создания записи\n",
    "Можно считать что это дата расчета переменных\n",
    "Остальные поля описывать нет смысла\n",
    "Это поля - характеристики кредитной истории, что они значат не важно, важно как они влияют на целевое событие\n",
    "А также таблицы 2 и 3 между собой нельзя соединить\n",
    "45 джини это идеально\n",
    "Можно и меньше, чтобы присылали решения с меньшими результатами)\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "from tqdm import tqdm_notebook\n",
    "import os, re, sys, gc, pickle, time\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "from plot_metric.functions import BinaryClassification\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "from tqdm import tqdm_notebook\n",
    "import os, re, sys, gc, pickle, time\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score,\\\n",
    "                                    KFold, train_test_split, cross_validate, ParameterGrid,\\\n",
    "                                    cross_validate, cross_val_predict, TimeSeriesSplit, StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.base import BaseEstimator, TransformerMixin,  clone\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# пайплайн\n",
    "from sklearn.pipeline import Pipeline, make_union, make_pipeline, FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin,  clone\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler, OneHotEncoder\n",
    "\n",
    "# дамми-регрессор\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from itertools import combinations\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import time\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import roc_curve\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_types(df):\n",
    "    '''конвертирует типы (по возможности)'''\n",
    "    df_c = df.copy()\n",
    "    for col in df.columns:\n",
    "        ser = df[col]\n",
    "        try:\n",
    "            ser2 = ser.astype('datetime64')\n",
    "            df_c[col] = ser2\n",
    "        except:\n",
    "            try:\n",
    "                ser_float =ser.astype(np.float32)\n",
    "                ser_int =ser.astype(np.int32)\n",
    "                if (ser_float == ser_int).all():\n",
    "                    df_c[col] =ser_int\n",
    "                else:\n",
    "                    df_c[col] =ser_float\n",
    "            except:\n",
    "                try:\n",
    "                    df_c[col] = ser.str.lower().str.strip()\n",
    "                except:\n",
    "                    df_c[col] = ser\n",
    "    return df_c\n",
    "\n",
    "def get_counts(ser):   \n",
    "    '''\n",
    "    считает доли категорий\n",
    "    '''\n",
    "    return ser.value_counts(normalize = True).T.add_prefix(f'{ser.name}__')    \n",
    "def get_ntop(ser, n):\n",
    "    '''\n",
    "    выбирает топ категорий\n",
    "    '''\n",
    "    return ser.value_counts().head(n).index\n",
    "\n",
    "def collect_features_v2(df_buffer_table2, idx_col):\n",
    "    '''\n",
    "    извлечение признаков 2\n",
    "    '''\n",
    "    inqAmount_5 = get_ntop(df_buffer_table2['inqAmount'], 5)\n",
    "    inqAmount_10 = get_ntop(df_buffer_table2['inqAmount'], 10)\n",
    "    inqAmount_20 = get_ntop(df_buffer_table2['inqAmount'], 20)\n",
    "    inqAmount_50 = get_ntop(df_buffer_table2['inqAmount'], 50)\n",
    "    inqAmount_100 = get_ntop(df_buffer_table2['inqAmount'], 100)\n",
    "    L_inqAmount = [inqAmount_5, inqAmount_10, inqAmount_20, inqAmount_50, inqAmount_100]\n",
    "\n",
    "    inqControlNum_5 = get_ntop(df_buffer_table2['inqControlNum'], 5)\n",
    "    inqControlNum_10 = get_ntop(df_buffer_table2['inqControlNum'], 10)\n",
    "    inqControlNum_20 = get_ntop(df_buffer_table2['inqControlNum'], 20)\n",
    "    inqControlNum_50 = get_ntop(df_buffer_table2['inqControlNum'], 50)\n",
    "    inqControlNum_100 = get_ntop(df_buffer_table2['inqControlNum'], 100)\n",
    "    L_inqControlNum = [inqControlNum_5, inqControlNum_10, inqControlNum_20, inqControlNum_50, inqControlNum_100]\n",
    "    \n",
    "    \n",
    "    idx_to_use = df_buffer_table3[idx_col].unique()\n",
    "    df_features= pd.DataFrame(index= idx_to_use)\n",
    "    for REQUEST_ID, subdf in tqdm_notebook(df_buffer_table3.groupby(idx_col)):\n",
    "\n",
    "        df_features.loc[REQUEST_ID,'nrows']=subdf.shape[0]\n",
    "        \n",
    "        df_features.loc[REQUEST_ID, 'inqAmount__nunique'] = subdf['inqAmount'].nunique()\n",
    "        df_features.loc[REQUEST_ID, 'inqControlNum__nunique'] = subdf['inqControlNum'].nunique()     \n",
    "        \n",
    "        row_inqAmount= pd.Series([subdf['inqAmount'].map(lambda x: x in inqAmount).mean()\\\n",
    "                                 for inqAmount in L_inqAmount]).to_frame().T.add_prefix('inqAmount__') \n",
    "        row_inqControlNum= pd.Series([subdf['inqControlNum'].map(lambda x: x in inqControlNum).mean()\\\n",
    "                                 for inqControlNum in L_inqControlNum]).to_frame().T.add_prefix('inqControlNum__')\n",
    "        df_features.loc[REQUEST_ID,row_inqAmount.columns]=row_inqAmount.values\n",
    "        df_features.loc[REQUEST_ID,row_inqControlNum.columns]=row_inqControlNum.values\n",
    "        \n",
    "        for col in ['inqPurpose', 'inquiryPeriod', 'inqPurposeText']:        \n",
    "            shares = get_counts(subdf[col]).to_frame(REQUEST_ID).T\n",
    "            df_features.loc[REQUEST_ID,shares.columns]=shares.values\n",
    "            \n",
    "    return df_features\n",
    "\n",
    "\n",
    "def collect_features_v1(df_buffer_table2, idx_col):\n",
    "    '''\n",
    "    извлечение признаков 1\n",
    "    '''\n",
    "    paymtPat_5 = get_ntop(df_buffer_table2['paymtPat'], 5)\n",
    "    paymtPat_10 = get_ntop(df_buffer_table2['paymtPat'], 10)\n",
    "    paymtPat_20 = get_ntop(df_buffer_table2['paymtPat'], 20)\n",
    "    paymtPat_50 = get_ntop(df_buffer_table2['paymtPat'], 50)\n",
    "    paymtPat_100 = get_ntop(df_buffer_table2['paymtPat'], 100)\n",
    "    L_paymtPat = [paymtPat_5, paymtPat_10, paymtPat_20, paymtPat_50, paymtPat_100]\n",
    "\n",
    "    monthsReviewed_5 = get_ntop(df_buffer_table2['monthsReviewed'], 5)\n",
    "    monthsReviewed_10 = get_ntop(df_buffer_table2['monthsReviewed'], 10)\n",
    "    monthsReviewed_20 = get_ntop(df_buffer_table2['monthsReviewed'], 20)\n",
    "    monthsReviewed_50 = get_ntop(df_buffer_table2['monthsReviewed'], 50)\n",
    "    monthsReviewed_100 = get_ntop(df_buffer_table2['monthsReviewed'], 100)\n",
    "    L_monthsReviewed = [monthsReviewed_5, monthsReviewed_10, monthsReviewed_20, monthsReviewed_50, monthsReviewed_100]\n",
    "    \n",
    "    idx_to_use = df_buffer_table2[idx_col].unique()\n",
    "    df_features= pd.DataFrame(index= idx_to_use)\n",
    "    for REQUEST_ID, subdf in tqdm_notebook(df_buffer_table2.groupby(idx_col)):\n",
    "\n",
    "        df_features.loc[REQUEST_ID,'nrows']=subdf.shape[0]\n",
    "\n",
    "        row_paymtPat= pd.Series([subdf['paymtPat'].map(lambda x: x in paymtPat).mean()\\\n",
    "                                 for paymtPat in L_paymtPat]).to_frame().T.add_prefix('paymtPat__') \n",
    "        row_monthsReviewed= pd.Series([subdf['monthsReviewed'].map(lambda x: x in monthsReviewed).mean()\\\n",
    "                                 for monthsReviewed in L_monthsReviewed]).to_frame().T.add_prefix('monthsReviewed__')\n",
    "        df_features.loc[REQUEST_ID,row_paymtPat.columns]=row_paymtPat.values\n",
    "        df_features.loc[REQUEST_ID,row_monthsReviewed.columns]=row_monthsReviewed.values\n",
    "\n",
    "        df_features.loc[REQUEST_ID,'UUID_nanshare']= subdf['UUID'].isna().mean()\n",
    "\n",
    "        for col in ['currencyCode', 'paymtFreqText', 'interestPaymentFrequencyText',\n",
    "                    'accountRatingText', 'acctTypeText', 'BUSINESSCATEGORY',\\\n",
    "                    'accountRating', 'acctType', 'numDays60', 'numDays30', 'numDays90',\n",
    "                    'termsFrequency', 'interestPaymentFrequencyCode']:        \n",
    "            shares = get_counts(subdf[col]).to_frame(REQUEST_ID).T\n",
    "            df_features.loc[REQUEST_ID,shares.columns]=shares.values\n",
    "\n",
    "        for col in ['amtPastDue', 'curBalanceAmt', 'termsAmt', 'amtOutstanding', 'CREDITTOTALAMT', 'PRINCIPALOUTSTANDING',\n",
    "                    'PRINCIPALPASTDUE', 'OTHERAMTOUTSTANDING', 'OTHERAMTPASTDUE',\n",
    "                    'INTOUTSTANDING', 'INTPASTDUE', 'creditLimit', 'numDays60', 'numDays30', 'numDays90']:\n",
    "            df_features.loc[REQUEST_ID, '{}__sum'.format(str(col))] = subdf[col].sum()\n",
    "            df_features.loc[REQUEST_ID, '{}__mean'.format(str(col))] = subdf[col].mean()\n",
    "            df_features.loc[REQUEST_ID, '{}__max'.format(str(col))] = subdf[col].max()\n",
    "            df_features.loc[REQUEST_ID, '{}__min'.format(str(col))] = subdf[col].min()\n",
    "\n",
    "        for col in  ['openedDt', 'lastPaymtDt', 'closedDt', 'reportingDt', 'paymtPatStartDt',\n",
    "                     'lastUpdatedDt', 'accountRatingDate', 'paymentDueDate',\n",
    "                     'interestPaymentDueDate', 'DATE_RESPONSE', 'CREATE_TSTAMP',\n",
    "                     'ETL_TIMESTAMP', 'GRACEENDDT']:\n",
    "            year_shares = subdf[col].dt.year.value_counts(normalize= True)\n",
    "            month_shares = subdf[col].dt.month.value_counts(normalize= True)\n",
    "            day_shares = subdf[col].dt.day.value_counts(normalize= True)\n",
    "            dow_shares = subdf[col].dt.dayofweek.value_counts(normalize= True)\n",
    "            hour_shares = subdf[col].dt.round('H').dt.hour.value_counts(normalize= True)\n",
    "            for name, ser in zip(('year_shares', 'month_shares', 'day_shares', 'dow_shares', 'hour_shares'),\\\n",
    "                                 (year_shares, month_shares, day_shares, dow_shares, hour_shares)):\n",
    "                for k, v in ser.items():\n",
    "                    df_features.loc[REQUEST_ID, f'{name}__{k}'] = v    \n",
    "\n",
    "            lifetime = (subdf[col].max() - subdf[col].min())/np.timedelta64(1, 's')\n",
    "            df_features.loc[REQUEST_ID, 'lifetime'] = lifetime \n",
    "            \n",
    "    return df_features\n",
    "\n",
    "\n",
    "class SklearnHelperTargetEncoder(BaseEstimator, TransformerMixin):\n",
    "    ''' Кодирование категорий с помощью целевой переменной '''\n",
    "    def __init__(self, n_iter, n_folds, min_samples_leaf, seed):\n",
    "        self.n_iter = n_iter\n",
    "        self.n_folds = n_folds\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.seed = seed\n",
    "    def fit(self, X, y=None):\n",
    "        self.y_mean = y.mean()\n",
    "        _df_tr = pd.concat([X, y], 1)\n",
    "        target_col = _df_tr.columns[-1]\n",
    "        to_encode = _df_tr.columns[:-1]\n",
    "        \n",
    "        L_tr = []        \n",
    "        self.L_d_encs = []\n",
    "        for i in tqdm_notebook(range(self.n_iter)): \n",
    "            enc_tr = pd.DataFrame(index = _df_tr.index, columns = to_encode).fillna(0.0)\n",
    "            for col in to_encode:\n",
    "                for tr_idx, val_idx in KFold(self.n_folds, shuffle = True,random_state = self.seed+i)\\\n",
    "                                       .split(_df_tr):                    \n",
    "                    grp = _df_tr.iloc[tr_idx].groupby(col)[target_col].agg({'mean', 'count'}) \n",
    "                    d_enc = grp[grp['count']>=self.min_samples_leaf]['mean'].to_dict()\n",
    "                    self.L_d_encs.append((col, d_enc))\n",
    "                    to_enc_tr =_df_tr.iloc[val_idx]                    \n",
    "                    enc_tr.loc[to_enc_tr.index, col] = to_enc_tr[col].map(d_enc)                  \n",
    "            L_tr.append(enc_tr)    \n",
    "            \n",
    "        self.enc_tr =  pd.concat(L_tr, 1)\n",
    "        self._df_tr = _df_tr\n",
    "        return self    \n",
    "    def transform(self, X):\n",
    "        if np.all(X.values == self._df_tr.values):\n",
    "            return self.enc_tr.fillna(self.y_mean) \n",
    "        else:\n",
    "            df_enc = pd.DataFrame(index = X.index, columns=X.columns).fillna(0.0)\n",
    "            for feat, d in tqdm_notebook(self.L_d_encs):\n",
    "                df_enc.loc[:, feat] += X[feat].map(d) / self.n_iter\n",
    "            return df_enc.fillna(self.y_mean)        \n",
    "\n",
    "class SklearnHelperFeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    ''' Отбор признаков '''\n",
    "    def __init__(self, model, cv, scoring, show_progress):\n",
    "        self.model = model\n",
    "        self.cv = cv\n",
    "        self.scoring = scoring\n",
    "        self.show_progress = show_progress\n",
    "    def fit(self, X, y=None):\n",
    "        #assert (isinstance(X, np.ndarray)) or (X.getformat() == 'csc')\n",
    "        _X = X.copy()\n",
    "        cv_scores = []\n",
    "        for i in tqdm_notebook(range(_X.shape[1])):\n",
    "            try:\n",
    "                _X_curr = _X[:, i].toarray().reshape(-1,1)\n",
    "            except:\n",
    "                _X_curr = _X[:, i].reshape(-1,1)                \n",
    "            mean_cv_score = cross_val_score(self.model, _X_curr, y, cv =self.cv, scoring = self.scoring, n_jobs=-1).mean()            \n",
    "            cv_scores.append(mean_cv_score)\n",
    "            \n",
    "        order = np.argsort(cv_scores)[::-1]\n",
    "        to_drop_before, best_features, best_cv_score = [], [order[0]], -np.inf\n",
    "        for i in tqdm_notebook(order[1:]):\n",
    "            curr_features = best_features+[i]\n",
    "            _X_curr = _X[:, curr_features]\n",
    "            mean_cv_score = cross_val_score(self.model, _X_curr, y, cv =self.cv, scoring = self.scoring, n_jobs=-1).mean()\n",
    "            if mean_cv_score>best_cv_score:\n",
    "                best_cv_score = mean_cv_score\n",
    "                best_features = curr_features\n",
    "                if self.show_progress:\n",
    "                    print('new best score = {:.10f}'.format(best_cv_score))\n",
    "            else:\n",
    "                to_drop_before.append(i)\n",
    "        while True:\n",
    "            to_drop_after = []\n",
    "            for i in tqdm_notebook(to_drop_before):\n",
    "                curr_features = best_features+[i]\n",
    "                _X_curr = _X[:, curr_features]\n",
    "                mean_cv_score = cross_val_score(self.model, _X_curr, y, cv =self.cv, scoring = self.scoring, n_jobs=-1).mean()\n",
    "                if mean_cv_score>best_cv_score:\n",
    "                    best_cv_score = mean_cv_score\n",
    "                    best_features = curr_features\n",
    "                    if self.show_progress:\n",
    "                        print('new best score = {:.10f}'.format(best_cv_score))\n",
    "                else:\n",
    "                    to_drop_after.append(i)\n",
    "            if to_drop_before == to_drop_after:\n",
    "                break\n",
    "            else:\n",
    "                to_drop_before = to_drop_after  \n",
    "        self.best_features_ = best_features\n",
    "        self.best_score_ = best_cv_score\n",
    "    def transform(self, _X):\n",
    "        return _X[:, self.best_features_]\n",
    "    \n",
    "def gini(y_trues, y_predprob):\n",
    "    return (2 * roc_auc(y_trues, y_predprob[:, 1])) - 1\n",
    "gini_scorer = make_scorer(gini, larger_is_better=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Решение\n",
    "* признаки извлек в другой тетрадке с помощью функций: _collect_features_v1_, _collect_features_v2_ \n",
    "* извлекались счетчики, уникальные счетчики, суммы,средние, минимум, максимум"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('X_tr', 'rb') as f:\n",
    "    X_tr=pickle.load(f)    \n",
    "with open('y_tr', 'rb') as f:\n",
    "    y_tr=pickle.load(f)\n",
    "with open('X_te', 'rb') as f:\n",
    "    X_te=pickle.load(f)\n",
    "with open('y_te', 'rb') as f:\n",
    "    y_te=pickle.load(f)\n",
    "\n",
    "assert (X_tr.index== y_tr.index).all()\n",
    "assert (X_te.index== y_te.index).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 13\n",
    "NFOLDS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_use = list(set(X_tr.columns).intersection(set(X_te.columns)))\n",
    "X_tr = X_tr[cols_to_use]\n",
    "X_te = X_te[cols_to_use]\n",
    "\n",
    "nuniques = X_tr.nunique()\n",
    "X_tr = X_tr[nuniques[nuniques>1].index]\n",
    "X_te = X_te[nuniques[nuniques>1].index]\n",
    "\n",
    "X_tr.columns = range(len(X_tr.columns))\n",
    "X_te.columns = range(len(X_te.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuniques_v2 = X_tr.nunique()\n",
    "to_enc = nuniques_v2[nuniques_v2<=100].index.tolist()\n",
    "encoder = SklearnHelperTargetEncoder(n_iter=20, n_folds=20, min_samples_leaf=5, seed=SEED)\n",
    "\n",
    "encoder.fit(X_tr[to_enc], y_tr)\n",
    "X_enc_tr = encoder.transform(X_tr[to_enc])\n",
    "X_enc_te = encoder.transform(X_te[to_enc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80893621a82d4ae1b9615359ad7209d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SklearnHelperTargetEncoder(min_samples_leaf=5, n_folds=20, n_iter=20, seed=13)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58790d17e8cd45b8b8ed5291bba2769b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=89600.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cacdbd01508242f0a143ad6be92b9046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=89600.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_tr = np.column_stack([X_tr.values, X_enc_tr.values])\n",
    "X2_te = np.column_stack([X_te.values, X_enc_te.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X2_tr)\n",
    "_X2_tr = scaler.transform(X2_tr)\n",
    "_X2_te = scaler.transform(X2_te)\n",
    "_y_tr = y_tr.copy()\n",
    "_y_te = y_te.copy()\n",
    "_X2_tr[np.isnan(_X2_tr)] = -1\n",
    "_X2_te[np.isnan(_X2_te)] = -1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# валидация, модель\n",
    "SKF = StratifiedKFold(NFOLDS, random_state = SEED, shuffle = True)    \n",
    "logit= LogisticRegression(random_state = SEED) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97eb5fe8b1494bc8885bed2db701e7df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=743.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0346e1d175ca4f69bffb8a16acf953a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=742.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best score = 0.6340854176\n",
      "new best score = 0.6743846075\n",
      "new best score = 0.6940894347\n",
      "new best score = 0.7090809617\n",
      "new best score = 0.7181546969\n",
      "new best score = 0.7187592583\n",
      "new best score = 0.7236650335\n",
      "new best score = 0.7274228682\n",
      "new best score = 0.7371757232\n",
      "new best score = 0.7372691819\n",
      "new best score = 0.7376311009\n",
      "new best score = 0.7401619974\n",
      "new best score = 0.7402866981\n",
      "new best score = 0.7413180488\n",
      "new best score = 0.7416612761\n",
      "new best score = 0.7442770198\n",
      "new best score = 0.7467665774\n",
      "new best score = 0.7472056667\n",
      "new best score = 0.7541206800\n",
      "new best score = 0.7543168265\n",
      "new best score = 0.7547732724\n",
      "new best score = 0.7551857092\n",
      "new best score = 0.7573354108\n",
      "new best score = 0.7576994660\n",
      "new best score = 0.7577424737\n",
      "new best score = 0.7577498670\n",
      "new best score = 0.7587931504\n",
      "new best score = 0.7615546897\n",
      "new best score = 0.7616446605\n",
      "new best score = 0.7627206211\n",
      "new best score = 0.7632295040\n",
      "new best score = 0.7633362973\n",
      "new best score = 0.7660826663\n",
      "new best score = 0.7661988889\n",
      "new best score = 0.7671733466\n",
      "new best score = 0.7683793651\n",
      "new best score = 0.7686027148\n",
      "new best score = 0.7687844921\n",
      "new best score = 0.7696697635\n",
      "new best score = 0.7696977177\n",
      "new best score = 0.7698207828\n",
      "new best score = 0.7701688499\n",
      "new best score = 0.7702016773\n",
      "new best score = 0.7703790320\n",
      "new best score = 0.7706607768\n",
      "new best score = 0.7708132647\n",
      "new best score = 0.7710273854\n",
      "new best score = 0.7712507185\n",
      "new best score = 0.7712741666\n",
      "new best score = 0.7742150800\n",
      "new best score = 0.7746841762\n",
      "new best score = 0.7746999807\n",
      "new best score = 0.7759725051\n",
      "new best score = 0.7765571731\n",
      "new best score = 0.7767652523\n",
      "new best score = 0.7767962272\n",
      "new best score = 0.7778725717\n",
      "new best score = 0.7782754790\n",
      "new best score = 0.7789654216\n",
      "new best score = 0.7798206360\n",
      "new best score = 0.7798667645\n",
      "new best score = 0.7798958035\n",
      "new best score = 0.7799535977\n",
      "new best score = 0.7828203115\n",
      "new best score = 0.7846105640\n",
      "new best score = 0.7849145887\n",
      "new best score = 0.7852300621\n",
      "new best score = 0.7858673340\n",
      "new best score = 0.7864756336\n",
      "new best score = 0.7865175232\n",
      "new best score = 0.7865529708\n",
      "new best score = 0.7866047235\n",
      "new best score = 0.7866385189\n",
      "new best score = 0.7881942734\n",
      "new best score = 0.7891663279\n",
      "new best score = 0.7896431344\n",
      "new best score = 0.7897232920\n",
      "new best score = 0.7897851083\n",
      "new best score = 0.7910065641\n",
      "new best score = 0.7910482367\n",
      "new best score = 0.7911069155\n",
      "new best score = 0.7911153601\n",
      "new best score = 0.7912226708\n",
      "new best score = 0.7917340905\n",
      "new best score = 0.7917454557\n",
      "new best score = 0.7922192249\n",
      "new best score = 0.7923535218\n",
      "new best score = 0.7925830965\n",
      "new best score = 0.7927403576\n",
      "new best score = 0.7938211079\n",
      "new best score = 0.7938876806\n",
      "new best score = 0.7953573362\n",
      "new best score = 0.7956170681\n",
      "new best score = 0.7960012003\n",
      "new best score = 0.7961899536\n",
      "new best score = 0.7965385381\n",
      "new best score = 0.7967551621\n",
      "new best score = 0.7968420287\n",
      "new best score = 0.7977740127\n",
      "new best score = 0.7978955759\n",
      "new best score = 0.7980708277\n",
      "new best score = 0.7982310093\n",
      "new best score = 0.7987557803\n",
      "new best score = 0.7990114066\n",
      "new best score = 0.7995830905\n",
      "new best score = 0.7996316390\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "669dd68ef4ca47688e44ccde9a16f6c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=636.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new best score = 0.7996489622\n",
      "new best score = 0.7996606613\n",
      "new best score = 0.8004160250\n",
      "new best score = 0.8004928614\n",
      "new best score = 0.8007614552\n",
      "new best score = 0.8008178642\n",
      "new best score = 0.8010599391\n",
      "new best score = 0.8011332708\n",
      "new best score = 0.8011659480\n",
      "new best score = 0.8011856745\n",
      "new best score = 0.8016476278\n",
      "new best score = 0.8016922376\n",
      "new best score = 0.8017741142\n",
      "new best score = 0.8018078428\n",
      "new best score = 0.8018778033\n",
      "new best score = 0.8018814082\n",
      "new best score = 0.8023095494\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8693f49333f49648d92b05df3538b36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=619.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# отбираем признаки\n",
    "selector = SklearnHelperFeatureSelector(model=logit,\\\n",
    "                                        cv=SKF,\n",
    "                                        scoring='roc_auc',\\\n",
    "                                        show_progress = True)\n",
    "selector.fit(_X2_tr, _y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sel_tr = selector.transform(_X2_tr)\n",
    "x_sel_te = selector.transform(_X2_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=13, shuffle=True),\n",
       "             estimator=LogisticRegression(random_state=13),\n",
       "             param_grid={'C': array([1.00000000e-03, 1.14975700e-03, 1.32194115e-03, 1.51991108e-03,\n",
       "       1.74752840e-03, 2.00923300e-03, 2.31012970e-03, 2.65608778e-03,\n",
       "       3.05385551e-03, 3.51119173e-03, 4.03701726e-03, 4.64158883e-03,\n",
       "       5.33669923e-03, 6.13590727e-0...\n",
       "       4.03701726e+01, 4.64158883e+01, 5.33669923e+01, 6.13590727e+01,\n",
       "       7.05480231e+01, 8.11130831e+01, 9.32603347e+01, 1.07226722e+02,\n",
       "       1.23284674e+02, 1.41747416e+02, 1.62975083e+02, 1.87381742e+02,\n",
       "       2.15443469e+02, 2.47707636e+02, 2.84803587e+02, 3.27454916e+02,\n",
       "       3.76493581e+02, 4.32876128e+02, 4.97702356e+02, 5.72236766e+02,\n",
       "       6.57933225e+02, 7.56463328e+02, 8.69749003e+02, 1.00000000e+03])},\n",
       "             scoring='roc_auc')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs = GridSearchCV(logit, param_grid = {'C':np.logspace(-3,3,100)}, scoring = 'roc_auc', cv =SKF)\n",
    "gs.fit(x_sel_tr, _y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predprob_cv = cross_val_predict(gs.best_estimator_, x_sel_tr, _y_tr, cv = SKF, method='predict_proba')[:,1]\n",
    "y_predprob_te = gs.best_estimator_.predict_proba(x_sel_te)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "GINI_cv = 100*((2 * roc_auc_score(_y_tr, y_predprob_cv)) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60.49474767832157"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GINI_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Baseline – > 45 gini на train и test’е\n",
    "* Прислать нужно csv файл с 2 столбцами, 1 – ucdb_id, 2 – predict_proba – столбец с вероятностями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predprob_cv = pd.Series(y_predprob_cv).to_frame('predict_proba')\n",
    "df_predprob_cv['ucdb_id'] = np.int64(y_tr.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predprob_te = pd.Series(y_predprob_te).to_frame('predict_proba')\n",
    "df_predprob_te['ucdb_id'] = np.int64(y_te.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predprob_te.to_excel('predict_proba_TE.xlsx')\n",
    "df_predprob_cv.to_excel('predict_proba_CV(gini=60.5).xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([1068809235, 1069323927, 1070123601, 1069249312, 1062806411,\n",
       "            1064180804, 1065696254, 1062459772, 1069975500, 1070609484,\n",
       "            ...\n",
       "            1068285451, 1068693464, 1068621594, 1068359972, 1066715545,\n",
       "            1062391323, 1062331468, 1068496544, 1071147256, 1066893283],\n",
       "           dtype='int64', name='ucdb_id', length=2414)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_te.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predict_proba</th>\n",
       "      <th>ucdb_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.076383</td>\n",
       "      <td>1068809235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.016906</td>\n",
       "      <td>1069323927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.087695</td>\n",
       "      <td>1070123601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.076547</td>\n",
       "      <td>1069249312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.277512</td>\n",
       "      <td>1062806411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2409</th>\n",
       "      <td>0.035537</td>\n",
       "      <td>1062391323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2410</th>\n",
       "      <td>0.433791</td>\n",
       "      <td>1062331468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2411</th>\n",
       "      <td>0.023584</td>\n",
       "      <td>1068496544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2412</th>\n",
       "      <td>0.068864</td>\n",
       "      <td>1071147256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2413</th>\n",
       "      <td>0.000898</td>\n",
       "      <td>1066893283</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2414 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      predict_proba     ucdb_id\n",
       "0          0.076383  1068809235\n",
       "1          0.016906  1069323927\n",
       "2          0.087695  1070123601\n",
       "3          0.076547  1069249312\n",
       "4          0.277512  1062806411\n",
       "...             ...         ...\n",
       "2409       0.035537  1062391323\n",
       "2410       0.433791  1062331468\n",
       "2411       0.023584  1068496544\n",
       "2412       0.068864  1071147256\n",
       "2413       0.000898  1066893283\n",
       "\n",
       "[2414 rows x 2 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predprob_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
