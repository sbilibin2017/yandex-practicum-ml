{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вспомогательный функции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### выбирает колонки для пайплайна"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SklearnHelperColumnSelector(BaseEstimator, TransformerMixin):\n",
    "    '''выбирает колонки, отпавляемые в пайплайн'''\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return X[self.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### факторизация категорий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SklearnHelperLabelEncoder(TransformerMixin, BaseEstimator):\n",
    "    ''' Факторизация категорий '''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, X, y=None):\n",
    "        X_c = X.astype(str)\n",
    "        self.d1 = {}\n",
    "        for col in X_c.columns:\n",
    "            uniques = X_c[col].dropna().unique() \n",
    "            self.d1[col] =  dict(zip(uniques, range(len(uniques))))              \n",
    "        return self\n",
    "    def transform(self, X): \n",
    "        X_c = X.astype(str)\n",
    "        for key, value in self.d1.items():\n",
    "            X_c[key] = X_c[key].map(value)\n",
    "        return X_c\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### кодирование категорий с помощью целевой переменной"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SklearnHelperTargetEncoder(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Кодирование категорий с помощью целевой переменной\n",
    "    \n",
    "        1. разбиваем данные на фолды\n",
    "        2. делаем ооф оценку\n",
    "        3. повторяем n_iter раз        \n",
    "    '''\n",
    "    def __init__(self, n_iter, n_folds, min_samples_leaf, seed):\n",
    "        self.n_iter = n_iter\n",
    "        self.n_folds = n_folds\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.seed = seed\n",
    "    def fit(self, X, y=None):\n",
    "        self.y_mean = y.mean()\n",
    "        _df_tr = pd.concat([X, y], 1)\n",
    "        target_col = _df_tr.columns[-1]\n",
    "        to_encode = _df_tr.columns[:-1]\n",
    "        \n",
    "        L_tr = []        \n",
    "        self.L_d_encs = []\n",
    "        for i in tqdm_notebook(range(self.n_iter)): \n",
    "            enc_tr = pd.DataFrame(index = _df_tr.index, columns = to_encode).fillna(0.0)\n",
    "            for col in to_encode:\n",
    "                for tr_idx, val_idx in KFold(self.n_folds, shuffle = True,random_state = self.seed+i)\\\n",
    "                                       .split(_df_tr):                    \n",
    "                    grp = _df_tr.iloc[tr_idx].groupby(col)[target_col].agg({'mean', 'count'}) \n",
    "                    d_enc = grp[grp['count']>=self.min_samples_leaf]['mean'].to_dict()\n",
    "                    self.L_d_encs.append((col, d_enc))\n",
    "                    to_enc_tr =_df_tr.iloc[val_idx]                    \n",
    "                    enc_tr.loc[to_enc_tr.index, col] = to_enc_tr[col].map(d_enc)                  \n",
    "            L_tr.append(enc_tr)    \n",
    "            \n",
    "        self.enc_tr =  pd.concat(L_tr, 1)\n",
    "        self._df_tr = _df_tr\n",
    "        return self    \n",
    "    def transform(self, X):\n",
    "        if np.all(X.values == self._df_tr.values):\n",
    "            return self.enc_tr.fillna(self.y_mean) \n",
    "        else:\n",
    "            df_enc = pd.DataFrame(index = X.index, columns=X.columns).fillna(0.0)\n",
    "            for feat, d in tqdm_notebook(self.L_d_encs):\n",
    "                df_enc.loc[:, feat] += X[feat].map(d) / self.n_iter\n",
    "            return df_enc.fillna(self.y_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### разделение данных на 3 части"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hold_test_split(features, target, tr_size, ho_size, shuffle, random_state, stratify, use_test):\n",
    "    if use_test:\n",
    "        # делим данные на тренировочную, отложенную, тестовую части\n",
    "        features_trho, features_te, target_trho, target_te = train_test_split(\\\n",
    "                                                                       features, target,\\\n",
    "                                                                       train_size = tr_size,\\\n",
    "                                                                       shuffle = shuffle, random_state = random_state,\\\n",
    "                                                                       stratify = target if stratify else None)\n",
    "        features_tr, features_ho, target_tr, target_ho = train_test_split(\\\n",
    "                                                                       features_trho, target_trho,\\\n",
    "                                                                       train_size = 1-ho_size,\\\n",
    "                                                                       shuffle = shuffle, random_state = random_state,\\\n",
    "                                                                       stratify = target_trho if stratify else None)\n",
    "        return (features_tr, features_ho, features_te, target_tr, target_ho, target_te)\n",
    "    else:\n",
    "        # делим данные на тренировочную, отложенную, тестовую части\n",
    "        features_tr, features_ho, target_tr, target_ho = train_test_split(\\\n",
    "                                                             features, target,\\\n",
    "                                                             train_size = tr_size,\\\n",
    "                                                             shuffle = shuffle, random_state = random_state,\\\n",
    "                                                             stratify = target if stratify else None)\n",
    "        print('train size = {}, hold size ={}'\\\n",
    "              .format(features_tr.shape[0], features_ho.shape[0]))\n",
    "        return (features_tr, features_ho, target_tr, target_ho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Отбор признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SklearnHelperFeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Отбор признаков\n",
    "    \n",
    "        1. считаем валидацию каждого признака в отдельности\n",
    "        2. рекурсивно добавляем признаки в порядке убывания индивидуального качества\n",
    "        3. останавливаемся, когда ни один из признаков не был добавлен\n",
    "    '''\n",
    "    def __init__(self, model, cv, scoring, show_progress):\n",
    "        self.model = model\n",
    "        self.cv = cv\n",
    "        self.scoring = scoring\n",
    "        self.show_progress = show_progress\n",
    "    def fit(self, X, y=None):\n",
    "        #assert (isinstance(X, np.ndarray)) or (X.getformat() == 'csc')\n",
    "        _X = X.copy()\n",
    "        cv_scores = []\n",
    "        for i in tqdm_notebook(range(_X.shape[1])):\n",
    "            try:\n",
    "                _X_curr = _X[:, i].toarray().reshape(-1,1)\n",
    "            except:\n",
    "                _X_curr = _X[:, i].reshape(-1,1)                \n",
    "            mean_cv_score = cross_val_score(self.model, _X_curr, y, cv =self.cv, scoring = self.scoring, n_jobs=-1).mean()            \n",
    "            cv_scores.append(mean_cv_score)\n",
    "            \n",
    "        order = np.argsort(cv_scores)[::-1]\n",
    "        to_drop_before, best_features, best_cv_score = [], [order[0]], -np.inf\n",
    "        for i in tqdm_notebook(order[1:]):\n",
    "            curr_features = best_features+[i]\n",
    "            _X_curr = _X[:, curr_features]\n",
    "            mean_cv_score = cross_val_score(self.model, _X_curr, y, cv =self.cv, scoring = self.scoring, n_jobs=-1).mean()\n",
    "            if mean_cv_score>best_cv_score:\n",
    "                best_cv_score = mean_cv_score\n",
    "                best_features = curr_features\n",
    "                if self.show_progress:\n",
    "                    print('new best score = {:.10f}'.format(best_cv_score))\n",
    "            else:\n",
    "                to_drop_before.append(i)\n",
    "        while True:\n",
    "            to_drop_after = []\n",
    "            for i in tqdm_notebook(to_drop_before):\n",
    "                curr_features = best_features+[i]\n",
    "                _X_curr = _X[:, curr_features]\n",
    "                mean_cv_score = cross_val_score(self.model, _X_curr, y, cv =self.cv, scoring = self.scoring, n_jobs=-1).mean()\n",
    "                if mean_cv_score>best_cv_score:\n",
    "                    best_cv_score = mean_cv_score\n",
    "                    best_features = curr_features\n",
    "                    if self.show_progress:\n",
    "                        print('new best score = {:.10f}'.format(best_cv_score))\n",
    "                else:\n",
    "                    to_drop_after.append(i)\n",
    "            if to_drop_before == to_drop_after:\n",
    "                break\n",
    "            else:\n",
    "                to_drop_before = to_drop_after  \n",
    "        self.best_features_ = best_features\n",
    "        self.best_score_ = best_cv_score\n",
    "    def transform(self, _X):\n",
    "        return _X[:, self.best_features_]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### оптимизация гиперпараметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SklearnHelperClassifierHPTuner(BaseEstimator, TransformerMixin):  \n",
    "    '''\n",
    "    Оптимизация гиперпараметров моделей\n",
    "    '''\n",
    "    def __init__(self, model, cv, scoring):\n",
    "        self.model = model\n",
    "        self.cv = cv\n",
    "        self.scoring = scoring\n",
    "    def info(self):\n",
    "        pass\n",
    "    def fit(self, X, y=None):\n",
    "        #assert (isinstance(X, np.ndarray)) or (X.getformat() == 'csc')\n",
    "        best_estimator_ = clone(self.model)\n",
    "        best_params = {}\n",
    "        if type(self.model).__name__ == 'LGBMClassifier':     \n",
    "            init_params = self.model.get_params()\n",
    "            bp = {'n_estimators':init_params['n_estimators'],\\\n",
    "                  'random_state':init_params['random_state'],\\\n",
    "                  'n_jobs':init_params['n_jobs']}\n",
    "            \n",
    "            gs = GridSearchCV(best_estimator_,\\\n",
    "                              param_grid = {'n_estimators':[10], 'n_jobs':[-1], 'random_state':[SEED],\\\n",
    "                                            'max_depth':np.arange(2, 21).tolist(),\\\n",
    "                                            'num_leaves':[32, 64, 128, 256, 512, 1024],\\\n",
    "                                            'min_child_samples':[20, 50]},\\\n",
    "                              cv = self.cv,\\\n",
    "                              scoring=self.scoring,\\\n",
    "                              n_jobs=-1, verbose=1)\n",
    "            gs.fit(X, y)\n",
    "            best_params.update(gs.best_params_)\n",
    "            best_estimator_ = best_estimator_.set_params(**best_params)\n",
    "            \n",
    "            gs = GridSearchCV(best_estimator_,\\\n",
    "                              param_grid = {'subsample':np.linspace(.1, 1, 10),\\\n",
    "                                            'colsample_bytree':np.linspace(.1, 1, 10)},\\\n",
    "                              cv = self.cv,\\\n",
    "                              scoring=self.scoring,\\\n",
    "                              n_jobs=-1, verbose=1)\n",
    "            gs.fit(X, y)\n",
    "            best_params.update(gs.best_params_)\n",
    "            best_params['n_estimators'] = bp['n_estimators']\n",
    "            best_params['random_state'] = bp['random_state']\n",
    "            best_params['n_jobs'] = bp['n_jobs']\n",
    "            best_estimator_ = best_estimator_.set_params(**best_params)\n",
    "            \n",
    "            learning_rates = [.005,.006, .007, .008, .009,\\\n",
    "                              .01, .02, .03, .04, .05, .06, .07, .08, .09,\\\n",
    "                              .1, .2, .3, .4, .5]\n",
    "            best_score = -np.inf\n",
    "            for lr in tqdm_notebook(learning_rates):\n",
    "                best_params['learning_rate'] = lr\n",
    "                lgb_curr = best_estimator_.set_params(**best_params)\n",
    "                mean_cv_score = cross_val_score(lgb_curr, X, y, cv = self.cv, scoring = self.scoring).mean()\n",
    "                if mean_cv_score>best_score:\n",
    "                    best_score = mean_cv_score\n",
    "                    best_lr = lr\n",
    "                else:\n",
    "                    break\n",
    "            best_params['learning_rate'] = best_lr\n",
    "            self.best_estimator_ = best_estimator_.set_params(**best_params)\n",
    "            self.best_score_ =  best_score                   \n",
    "        elif type(self.model).__name__ == 'XGBClassifier': \n",
    "            init_params = best_estimator_.get_params()\n",
    "            bp = {'n_estimators':init_params['n_estimators'],\\\n",
    "                  'random_state':init_params['random_state'],\\\n",
    "                  'n_jobs':init_params['n_jobs']}\n",
    "            \n",
    "            gs = GridSearchCV(best_estimator_,\\\n",
    "                              param_grid = {'n_estimators':[10], 'n_jobs':[-1], 'random_state':[SEED],\\\n",
    "                                            'max_depth':np.arange(2, 21).tolist(),\\\n",
    "                                            'min_child_weight':[20, 50]},\\\n",
    "                              cv = self.cv,\\\n",
    "                              scoring=self.scoring,\\\n",
    "                              n_jobs=-1, verbose=1)\n",
    "            gs.fit(X, y)\n",
    "            best_params.update(gs.best_params_)            \n",
    "            best_estimator_ = best_estimator_.set_params(**best_params)\n",
    "            \n",
    "            gs = GridSearchCV(best_estimator_,\\\n",
    "                              param_grid = {'subsample':[.5, .6, .7, .8, .9, 1],\\\n",
    "                                            'colsample_bytree':[.5, .6, .7, .8, .9, 1]},\\\n",
    "                              cv = self.cv,\\\n",
    "                              scoring=self.scoring,\\\n",
    "                              n_jobs=-1, verbose=1)\n",
    "            gs.fit(X, y)\n",
    "            best_params.update(gs.best_params_)\n",
    "            best_params['n_estimators'] = bp['n_estimators']\n",
    "            best_params['random_state'] = bp['random_state']\n",
    "            best_params['n_jobs'] = bp['n_jobs']\n",
    "            best_estimator_ = best_estimator_.set_params(**best_params)            \n",
    "            \n",
    "            learning_rates = [.005,.006, .007, .008, .009,\\\n",
    "                              .01, .02, .03, .04, .05, .06, .07, .08, .09,\\\n",
    "                              .1, .2, .3, .4, .5]\n",
    "            best_score = -np.inf\n",
    "            for lr in tqdm_notebook(learning_rates):\n",
    "                best_params['learning_rate'] = lr\n",
    "                xgb_curr = best_estimator_.set_params(**best_params)\n",
    "                mean_cv_score = cross_val_score(xgb_curr, X, y, cv = self.cv, scoring = self.scoring).mean()\n",
    "                if mean_cv_score>best_score:\n",
    "                    best_score = mean_cv_score\n",
    "                    best_lr = lr                    \n",
    "                else:\n",
    "                    break            \n",
    "            best_params['learning_rate'] = best_lr\n",
    "            self.best_estimator_ = best_estimator_.set_params(**best_params)\n",
    "            self.best_score_ =  best_score                 \n",
    "        elif type(self.model).__name__ in ('DecisionTreeClassifier', 'ExtraTreeClassifier'):\n",
    "            gs = GridSearchCV(best_estimator_,\\\n",
    "                              param_grid = {'max_depth':np.arange(7, 41), 'min_samples_leaf':[2, 20, 200]},\\\n",
    "                              cv = self.cv,\\\n",
    "                              scoring=self.scoring,\\\n",
    "                              n_jobs=-1, verbose=1)\n",
    "            gs.fit(X, y)\n",
    "            best_params.update(gs.best_params_)\n",
    "            self.best_estimator_ = best_estimator_.set_params(**best_params)\n",
    "            self.best_score_ = gs.best_score_\n",
    "        elif type(self.model).__name__ in ('RandomForestClassifier', 'ExtraTreesClassifier'):\n",
    "            init_params = self.model.get_params()\n",
    "            bp = {'n_estimators':init_params['n_estimators'],\\\n",
    "                  'random_state':init_params['random_state'],\\\n",
    "                  'n_jobs':init_params['n_jobs']}\n",
    "            gs = GridSearchCV(best_estimator_,\\\n",
    "                              param_grid = {'max_depth':np.arange(5, 21),'min_samples_leaf':[2, 20],\\\n",
    "                                            'n_estimators':[10], 'n_jobs':[-1], 'random_state':[bp['random_state']]},\\\n",
    "                              cv = self.cv,\\\n",
    "                              scoring=self.scoring,\\\n",
    "                              n_jobs=-1, verbose=1)\n",
    "            gs.fit(X, y)\n",
    "            best_params.update(gs.best_params_)\n",
    "            best_params['n_estimators'] = bp['n_estimators']\n",
    "            best_params['random_state'] = bp['random_state']\n",
    "            best_params['n_jobs'] = bp['n_jobs']\n",
    "            self.best_estimator_ = best_estimator_.set_params(**best_params)\n",
    "            self.best_score_ = cross_val_score(self.best_estimator_,\\\n",
    "                                               X, y,\\\n",
    "                                               cv = self.cv, scoring=self.scoring, n_jobs=-1).mean()\n",
    "        elif type(self.model).__name__ in ('LogisticRegression'):            \n",
    "            gs = GridSearchCV(best_estimator_,\\\n",
    "                              param_grid = {'C':[.001, .002, .003, .004, .005,\\\n",
    "                                                     .01, .02, .03, .04, .05, .06, .07, .08, .09,\\\n",
    "                                                     .1, .2, .3, .4, .5, .6, .7, .8, .9,\\\n",
    "                                                     1, 2, 3, 4, 5]},\\\n",
    "                              cv = self.cv,\\\n",
    "                              scoring=self.scoring,\\\n",
    "                              n_jobs=-1, verbose=1)\n",
    "            gs.fit(X, y)\n",
    "            self.best_estimator_ = gs.best_estimator_\n",
    "            self.best_score_ = gs.best_score_\n",
    "            \n",
    "        elif type(self.model).__name__ == 'LinearSVC':\n",
    "            gs = GridSearchCV(best_estimator_,\\\n",
    "                              param_grid = {'C':[.5, 1, 2, 4, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100,\\\n",
    "                                                 150, 200, 250, 300, 350, 400, 450, 500]},\\\n",
    "                              cv = self.cv,\\\n",
    "                              scoring=self.scoring,\\\n",
    "                              n_jobs=-1, verbose=1)\n",
    "            gs.fit(X, y)\n",
    "            self.best_estimator_ = gs.best_estimator_\n",
    "            self.best_score_ = gs.best_score_\n",
    "            \n",
    "        elif type(self.model).__name__ == 'KNeighborsClassifier':\n",
    "            gs = GridSearchCV(best_estimator_,\\\n",
    "                              param_grid = {'n_neighbors':range(2, 11)},\\\n",
    "                              cv = self.cv,\\\n",
    "                              scoring=self.scoring,\\\n",
    "                              n_jobs=-1, verbose=1)\n",
    "            gs.fit(X, y)\n",
    "            self.best_estimator_ = gs.best_estimator_\n",
    "            self.best_score_ = gs.best_score_\n",
    "            \n",
    "        self.best_estimator_.fit(X, y) \n",
    "        try:\n",
    "            self.coef_imp_ = self.best_estimator_.coef_.flatten()\n",
    "        except:\n",
    "            try:\n",
    "                self.coef_imp_ = self.best_estimator_.feature_importances_.flatten()        \n",
    "            except:\n",
    "                self.coef_imp_ = None                \n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        return self.best_estimator_.predict(X) \n",
    "    \n",
    "class SklearnHelperStackingRegressor(BaseEstimator, TransformerMixin):\n",
    "    ''' Классический стекинг моделей '''\n",
    "    def __init__(self, L_base_models, nfolds, seed, path_to_folder):\n",
    "        self.L_base_models = L_base_models\n",
    "        self.nfolds = nfolds\n",
    "        self.seed = seed\n",
    "        self.path_to_folder=path_to_folder\n",
    "        \n",
    "        if os.path.isdir(self.path_to_folder):\n",
    "            shutil.rmtree(self.path_to_folder)\n",
    "            os.makedirs(self.path_to_folder)\n",
    "        else:\n",
    "            os.makedirs(self.path_to_folder) \n",
    "            \n",
    "    def fit(self, L_X, y=None):\n",
    "        L_Z = []\n",
    "        self.nrows = L_X[0].shape[0] \n",
    "        self.y_mean = y.mean()\n",
    "        # классический стекинг\n",
    "        for i, (model, X) in tqdm_notebook(enumerate(zip(self.L_base_models, L_X)),\\\n",
    "                                              total = len(self.L_base_models)):\n",
    "            current_seed=i+self.seed\n",
    "            kf = KFold(self.nfolds, random_state= current_seed, shuffle = True)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            # пустые таблицы\n",
    "            Z_tr = np.zeros((y.shape[0], 1))            \n",
    "\n",
    "            # запускаем фолдинг\n",
    "            for j, (tr_idx, val_idx) in tqdm_notebook(enumerate(kf.split(X, y)),\\\n",
    "                                                      total = self.nfolds):\n",
    "                model.fit(X[tr_idx], y[tr_idx])\n",
    "                \n",
    "                filename = os.path.join(self.path_to_folder, f'model_{i+1}_{j+1}.pickle')\n",
    "                with open(filename, 'wb') as f:\n",
    "                    pickle.dump((model, i, j), f)\n",
    "                    \n",
    "                Z_tr[val_idx, 0] = model.predict(X[val_idx])                \n",
    "\n",
    "            L_Z.append(Z_tr)\n",
    "            \n",
    "        self.Z = np.column_stack(L_Z)        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, L_X):\n",
    "        if self.nrows == L_X[0].shape[0]:\n",
    "            return self.Z     \n",
    "        else:  \n",
    "            folder_with_models = os.listdir(self.path_to_folder)\n",
    "            L = []\n",
    "            for file in tqdm_notebook(folder_with_models):    \n",
    "                with open(os.path.join(self.path_to_folder, file), 'rb') as f:\n",
    "                    model, i, j =pickle.load(f)\n",
    "                L.append(model.predict(L_X[i])) \n",
    "            XX_meta = np.column_stack(L)\n",
    "            X_meta = np.column_stack([arr.mean(1) for arr in np.array_split(XX_meta, len(self.L_base_models), axis = 1)])\n",
    "            \n",
    "            return X_meta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
